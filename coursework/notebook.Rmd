---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tools)

md5 <- md5sum('Coursera-SwiftKey.zip')
if (md5 != 'e0629c64b1747103a7e751b0a30d3858') {
   url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
   download.file(url, destfile = 'Coursera-SwiftKey.zip')
}
unzip('Coursera-SwiftKey.zip')

file.rename(from = './final/en_US/en_US.blogs.txt',    to = './en_US.blogs.txt')
file.rename(from = './final/en_US/en_US.news.txt',     to = './en_US.news.txt')
file.rename(from = './final/en_US/en_US.twitter.txt',  to = './en_US.twitter.txt')

unlink('./final/', recursive = TRUE, force = TRUE)
```

```{r}
```

```{r}
if(!require(slam)) {
  install.packages("slam",  type="binary")
  library(slam)
}
if(!require(tm)) {
  install.packages("tm")
  library(tm)
}
if(!require(Rgraphviz)) {
  source("http://bioconductor.org/biocLite.R")
  biocLite("Rgraphviz", ask = FALSE)
  library(Rgraphviz)
}
if(!require(wordcloud)) {
  install.packages("wordcloud")
  library(wordcloud)
}
```

### Tasks

```{r}
bannedWords <- as.list(read.csv(url('http://www.bannedwordlist.com/lists/swearWords.txt'), header = FALSE))
# head(bannedWords)
```


```{r}
LIMIT_NUM_DOCS <- 5000

con <- file('./en_US.blogs.txt') 
open(con)
v <- readLines(con, n=LIMIT_NUM_DOCS)
close(con)
vs <- VectorSource(v)

removeWordsContaining <-
function(x, words)
    UseMethod("removeWordsContaining", x)
removeWordsContaining.character <-
function(x, words)
    gsub(sprintf("(*UCP)\\b[a-zA-Z]*(%s)[a-zA-Z]*\\b",
                 paste(sort(words, decreasing = TRUE), collapse = "|")),
         "", x, perl = TRUE)
removeWordsContaining.PlainTextDocument <-
    content_transformer(removeWordsContaining.character)


blog_corpus <- Corpus(vs, readerControl = list(language="en")) 
blog_corpus <- tm_map(blog_corpus, content_transformer(tolower))
blog_corpus <- tm_map(blog_corpus, stripWhitespace) 
blog_corpus <- tm_map(blog_corpus, removePunctuation) 
blog_corpus <- tm_map(blog_corpus, removeNumbers) 
blog_corpus <- tm_map(blog_corpus, removeWords, c(stopwords("english"))) 
blog_corpus <- tm_map(blog_corpus, removeWordsContaining, bannedWords$V1) 
```

```{r}
if (LIMIT_NUM_DOCS < 200)
 wordcloud(blog_corpus)
```

```{r}
tdm <- TermDocumentMatrix(blog_corpus,
                          control = list(removePunctuation = TRUE,
                                         removeNumbers = TRUE,
                                         stopwords = TRUE))
inspect(tdm[1:10, 1:50])
```
```{r}
terms <- dimnames(tdm[ ,1:50])$Terms
inspect(tdm[grep('aadil', terms), 1:50])
inspect(tdm[grep('fuck', terms), 1:50])
```

```{r}
plot(tdm, corThreshold = 0.2, weighting = TRUE)
```

```{r}
# this seems to be reading blog_corpus meta
t <- MC_tokenizer(blog_corpus)
head(t, 100)
```

