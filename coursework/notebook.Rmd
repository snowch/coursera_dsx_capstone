---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tools)

md5 <- md5sum('Coursera-SwiftKey.zip')
if (md5 != 'e0629c64b1747103a7e751b0a30d3858') {
   url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
   download.file(url, destfile = 'Coursera-SwiftKey.zip')
}
unzip('Coursera-SwiftKey.zip')

file.rename(from = './final/en_US/en_US.blogs.txt',    to = './en_US.blogs.txt')
file.rename(from = './final/en_US/en_US.news.txt',     to = './en_US.news.txt')
file.rename(from = './final/en_US/en_US.twitter.txt',  to = './en_US.twitter.txt')

unlink('./final/', recursive = TRUE, force = TRUE)
```

```{r}
```

```{r}
if(!require(slam)) {
  install.packages("slam",  type="binary")
  library(slam)
}
if(!require(tm)) {
  install.packages("tm")
  library(tm)
}
if(!require(Rgraphviz)) {
  source("http://bioconductor.org/biocLite.R")
  biocLite("Rgraphviz", ask = FALSE)
  library(Rgraphviz)
}
if(!require(wordcloud)) {
  install.packages("wordcloud")
  library(wordcloud)
}
```

### Tasks

- Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
- Profanity filtering - removing profanity and other words you do not want to predict.


```{r}
con <- file('./en_US.blogs.txt') 
open(con)
v <- readLines(con, n=200)
close(con)
vs <- VectorSource(v)
blog_corpus <-Corpus(vs, readerControl = list(language="en")) 
#summary(blog_corpus)  #check what went in
```

```{r}
wordcloud(blog_corpus)
```

```{r}
tdm <- TermDocumentMatrix(blog_corpus,
                          control = list(removePunctuation = TRUE,
                                         removeNumbers = TRUE,
                                         stopwords = TRUE))
plot(tdm, corThreshold = 0.2, weighting = TRUE)
```

```{r}
# this seems to be reading blog_corpus meta
t <- MC_tokenizer(blog_corpus)
head(t, 100)
```

