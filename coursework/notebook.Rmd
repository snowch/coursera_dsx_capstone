---
title: "capstone"
author: "Chris Snow"
date: "06/11/2016"
output: html_document
---


```{r}
library(tools)

md5 <- md5sum('Coursera-SwiftKey.zip')
if (md5 != 'e0629c64b1747103a7e751b0a30d3858') {
   url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
   download.file(url, destfile = 'Coursera-SwiftKey.zip')
   unzip('Coursera-SwiftKey.zip')
   
   # free some space on my drive by removing unneeded files
   unlink('./final/de_DE/', recursive = TRUE, force = TRUE)
   unlink('./final/fi_FI/', recursive = TRUE, force = TRUE)
   unlink('./final/ru_RU/', recursive = TRUE, force = TRUE)
}

```


```{r}
if(!require(slam)) {
  install.packages("slam",  type="binary")
  library(slam)
}
if(!require(tm)) {
  install.packages("tm")
  library(tm)
}
if(!require(Rgraphviz)) {
  source("http://bioconductor.org/biocLite.R")
  biocLite("Rgraphviz", ask = FALSE)
  library(Rgraphviz)
}
if(!require(wordcloud)) {
  install.packages("wordcloud")
  library(wordcloud)
}
#if(!require(rJava)) {
#  install.packages("rJava","http://rforge.net/",type="source")
#  library(rJava)
#}
#if(!require(RWeka)) {
#  install.packages("RWeka")
#  library(RWeka)
#}
```

### Tasks

```{r}
bannedWords <- as.list(read.csv(url('http://www.bannedwordlist.com/lists/swearWords.txt'), header = FALSE))
# head(bannedWords)
```


```{r}
LIMIT_NUM_DOCS <- 5000

con <- file('./en_US.blogs.txt') 
open(con)
v <- readLines(con, n=LIMIT_NUM_DOCS)
close(con)
vs <- VectorSource(v)

removeWordsContaining <-
function(x, words)
    UseMethod("removeWordsContaining", x)
removeWordsContaining.character <-
function(x, words)
    gsub(sprintf("(*UCP)\\b[a-zA-Z]*(%s)[a-zA-Z]*\\b",
                 paste(sort(words, decreasing = TRUE), collapse = "|")),
         "", x, perl = TRUE)
removeWordsContaining.PlainTextDocument <-
    content_transformer(removeWordsContaining.character)

blog_corpus <- Corpus(vs, readerControl = list(language="en")) 
blog_corpus <- tm_map(blog_corpus, content_transformer(tolower))
blog_corpus <- tm_map(blog_corpus, stripWhitespace) 
blog_corpus <- tm_map(blog_corpus, removePunctuation) 
blog_corpus <- tm_map(blog_corpus, removeNumbers) 
blog_corpus <- tm_map(blog_corpus, removeWords, c(stopwords("english"))) 
blog_corpus <- tm_map(blog_corpus, removeWordsContaining, bannedWords$V1) 
```

```{r}
if (LIMIT_NUM_DOCS < 200)
 wordcloud(blog_corpus)
```

```{r}
tdm <- TermDocumentMatrix(blog_corpus,
                          control = list(removePunctuation = TRUE,
                                         removeNumbers = TRUE,
                                         stopwords = TRUE))
inspect(tdm[1:10, 1:25])
```
```{r}
terms <- dimnames(tdm[ ,1:50])$Terms
```

```{r}
inspect(tdm[grep('aadil', terms), 1:25])
```

```{r}
inspect(tdm[grep('fuck', terms), 1:25])
```

```{r}
plot(tdm, corThreshold = 0.2, weighting = TRUE)
```

```{r}
# this seems to be reading blog_corpus meta
t <- MC_tokenizer(blog_corpus)
head(t, 100)
```

```{r}
TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)

datmat<- DocumentTermMatrix(blog_corpus, control = list(tokenize = TrigramTokenizer))

dat<- as.matrix(datmat)
```

```{r}
dat[1:5, 1:5]
```