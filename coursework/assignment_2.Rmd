---
title: "Assignment 1 - exploratory analysis"
author: "Chris Snow"
date: "06/11/2016"
output: html_document
---

```{r load_raw_data, message=FALSE, results="hide", echo=FALSE}
LIMIT_NUM_DOCS <- -1

library(tools)

md5 <- md5sum('Coursera-SwiftKey.zip')
if (md5 != 'e0629c64b1747103a7e751b0a30d3858') {
   url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
   download.file(url, destfile = 'Coursera-SwiftKey.zip')
   unzip('Coursera-SwiftKey.zip')
   
   # free some space on my drive by removing unneeded files
   unlink('./final/de_DE/', recursive = TRUE, force = TRUE)
   unlink('./final/fi_FI/', recursive = TRUE, force = TRUE)
   unlink('./final/ru_RU/', recursive = TRUE, force = TRUE)
}

rm(md5)

if(!require(slam)) {
  install.packages("slam",  type="binary")
  library(slam)
}
if(!require(tm)) {
  install.packages("tm")
  library(tm)
}
if(!require(Rgraphviz)) {
  source("http://bioconductor.org/biocLite.R")
  biocLite("Rgraphviz", ask = FALSE)
  library(Rgraphviz)
}
if(!require(wordcloud)) {
  install.packages("wordcloud")
  library(wordcloud)
}
if(!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}

# Create the corpus

## blogs
con <- file('./final/en_US/en_US.blogs.txt') 
open(con)
v <- readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE)
close(con)

# news
con <- file('./final/en_US/en_US.news.txt') 
open(con)
v <- append(v, readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE))
close(con)

# twitter
con <- file('./final/en_US/en_US.twitter.txt') 
open(con)
v <- append(v, readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE))
close(con)

vs <- VectorSource(v)

rm(v, con)

bannedWords <- as.list(read.csv(url('http://www.bannedwordlist.com/lists/swearWords.txt'), header = FALSE))
# head(bannedWords)

blog_corpus <- Corpus(vs, readerControl = list(language="en")) 
blog_corpus <- tm_map(blog_corpus, content_transformer(tolower))
blog_corpus <- tm_map(blog_corpus, stripWhitespace) 
blog_corpus <- tm_map(blog_corpus, removePunctuation) 
blog_corpus <- tm_map(blog_corpus, removeNumbers) 

# the most common n-gram models are based on stop words - we don't want to remove them
# blog_corpus <- tm_map(blog_corpus, removeWords, c(stopwords("english")))

# news corpus has some unicode characters that need to be removed
ct <- content_transformer(function(x, pattern) gsub(pattern, "", x))
blog_corpus <- tm_map(blog_corpus, ct, "\u0095")
blog_corpus <- tm_map(blog_corpus, ct, "\u0096")
blog_corpus <- tm_map(blog_corpus, ct, "\u0097")

# create function to remove words containing a banned word, e.g. *word*

removeWordsContaining <-
function(x, words)
    UseMethod("removeWordsContaining", x)
removeWordsContaining.character <-
function(x, words)
    gsub(sprintf("(*UCP)\\b[a-zA-Z]*(%s)[a-zA-Z]*\\b",
                 paste(sort(words, decreasing = TRUE), collapse = "|")),
         "", x, perl = TRUE)
removeWordsContaining.PlainTextDocument <-
    content_transformer(removeWordsContaining.character)

# filter out the banned words using out removeWordsContaining function
blog_corpus <- tm_map(blog_corpus, removeWordsContaining, bannedWords$V1)

rm(vs, bannedWords, LIMIT_NUM_DOCS)

ngramDTM <- function (ngrams) {
  TrigramTokenizer <-
    function(x)
      unlist(lapply(ngrams(words(x), ngrams), paste, collapse = " "), use.names = FALSE)
  
  datmat <- TermDocumentMatrix(blog_corpus, control = list(tokenize = TrigramTokenizer))
  
  return(as.matrix(datmat))
}
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
digrams = ngramDTM(2)

trigrams = ngramDTM(3)

trigramNames$Terms[ grep('^case of.*', trigramNames$Terms) ]
```