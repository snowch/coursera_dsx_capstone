---
title: "Assignment 1 - exploratory analysis"
author: "Chris Snow"
date: "06/11/2016"
output: html_document
---

```{r load_raw_data, message=FALSE, results="hide", echo=FALSE}

LIMIT_NUM_DOCS <- -1

library(tools)

md5_blog = md5sum('./final/en_US/en_US.blogs.txt')[1]
md5_news = md5sum('./final/en_US/en_US.news.txt')[1]
md5_twit = md5sum('./final/en_US/en_US.twitter.txt')[1]

if (is.na(md5_blog) || md5_blog != '3d2df09d9d63c6da7aed897e3cdba908' ||
    is.na(md5_news) || md5_news != 'e066f2a551dbab595bf654977b3ac721' ||
    is.na(md5_twit) || md5_twit != '5afdd6d50b081c8037699a85d298f17f'
    ) {
  
   unlink('final/', recursive = TRUE)
   url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
   download.file(url, destfile = 'Coursera-SwiftKey.zip')
   unzip('Coursera-SwiftKey.zip',
         files=c('final/en_US/en_US.twitter.txt',
                 'final/en_US/en_US.blogs.txt',
                 'final/en_US/en_US.news.txt'))
   unlink('Coursera-SwiftKey.zip')
}

rm(md5_blog, md5_news, md5_twit)

if(!require(slam)) {
  install.packages("slam",  type="binary")
  library(slam)
}
if(!require(tm)) {
  install.packages("tm")
  library(tm)
}
#if(!require(Rgraphviz)) {
#  source("https://bioconductor.org/biocLite.R")
#  biocLite("Rgraphviz", ask = FALSE)
#  library(Rgraphviz)
#}
if(!require(wordcloud)) {
  install.packages("wordcloud")
  library(wordcloud)
}
if(!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}

# Create the corpus

## blogs
con <- file('./final/en_US/en_US.blogs.txt') 
open(con)
v <- readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE)
close(con)

# news
con <- file('./final/en_US/en_US.news.txt') 
open(con)
v <- append(v, readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE))
close(con)

# twitter
con <- file('./final/en_US/en_US.twitter.txt') 
open(con)
v <- append(v, readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE))
close(con)

vs <- VectorSource(v)

rm(v, con)

bannedWords <- as.list(read.csv(url('http://www.bannedwordlist.com/lists/swearWords.txt'), header = FALSE))
# head(bannedWords)

blog_corpus <- Corpus(vs, readerControl = list(language="en")) 
blog_corpus <- tm_map(blog_corpus, content_transformer(tolower))
blog_corpus <- tm_map(blog_corpus, stripWhitespace) 
blog_corpus <- tm_map(blog_corpus, removePunctuation) 
blog_corpus <- tm_map(blog_corpus, removeNumbers) 

# the most common n-gram models are based on stop words - we don't want to remove them
# blog_corpus <- tm_map(blog_corpus, removeWords, c(stopwords("english")))

# news corpus has some unicode characters that need to be removed
ct <- content_transformer(function(x, pattern) gsub(pattern, "", x))
blog_corpus <- tm_map(blog_corpus, ct, "\u0095")
blog_corpus <- tm_map(blog_corpus, ct, "\u0096")
blog_corpus <- tm_map(blog_corpus, ct, "\u0097")

# create function to remove words containing a banned word, e.g. *word*

removeWordsContaining <-
function(x, words)
    UseMethod("removeWordsContaining", x)
removeWordsContaining.character <-
function(x, words)
    gsub(sprintf("(*UCP)\\b[a-zA-Z]*(%s)[a-zA-Z]*\\b",
                 paste(sort(words, decreasing = TRUE), collapse = "|")),
         "", x, perl = TRUE)
removeWordsContaining.PlainTextDocument <-
    content_transformer(removeWordsContaining.character)

# filter out the banned words using out removeWordsContaining function
blog_corpus <- tm_map(blog_corpus, removeWordsContaining, bannedWords$V1)

rm(vs, bannedWords, LIMIT_NUM_DOCS)

ngramDTM <- function (ngrams) {
  TrigramTokenizer <-
    function(x)
      unlist(lapply(ngrams(words(x), ngrams), paste, collapse = " "), use.names = FALSE)
  
  datmat <- TermDocumentMatrix(blog_corpus, control = list(tokenize = TrigramTokenizer))
  
  return(as.matrix(datmat))
}
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
digrams = ngramDTM(2)

trigrams = ngramDTM(3)

trigramNames$Terms[ grep('^case of.*', trigramNames$Terms) ]
```
