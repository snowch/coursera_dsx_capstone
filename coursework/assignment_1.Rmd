---
title: "Assignment 1 - exploratory analysis"
author: "Chris Snow"
date: "06/11/2016"
output: html_document
---

# Overview

This report is an exploratory analysis of three data files containing blog, news and twitter data.  This analysis is the first step in a project to build a predictive typing model that helps suggests words and spellings as users are typing text into a device such as a mobile phone. 

This report contains only a high level summary of the exploratory analysis.  For those interested in more information, the markdown source for this report can be found here: https://github.com/snowch/coursera_dsx_capstone/blob/master/coursework/assignment_1.Rmd

# Summary Statistics

In this section, we provide some summary statistics of the data.

```{r load_raw_data, message=FALSE, results="hide", echo=FALSE}
library(tools)

md5 <- md5sum('Coursera-SwiftKey.zip')
if (md5 != 'e0629c64b1747103a7e751b0a30d3858') {
   url <- 'https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip'
   download.file(url, destfile = 'Coursera-SwiftKey.zip')
   unzip('Coursera-SwiftKey.zip')
   
   # free some space on my drive by removing unneeded files
   unlink('./final/de_DE/', recursive = TRUE, force = TRUE)
   unlink('./final/fi_FI/', recursive = TRUE, force = TRUE)
   unlink('./final/ru_RU/', recursive = TRUE, force = TRUE)
}

rm(md5)
```

## File statistics

We start with some statistics on the raw data files.

```{r unix_wc, message=FALSE, echo=FALSE}

# This is non portable as it requires unix wordcount (wc) utility, however it's very simple:

system('wc ./final/en_US/en_US.blogs.txt', intern = TRUE)
system('wc ./final/en_US/en_US.news.txt', intern = TRUE)
system('wc ./final/en_US/en_US.twitter.txt', intern = TRUE)
```

The output is:  number of lines, number of words, number of characters

```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
if(!require(slam)) {
  install.packages("slam",  type="binary")
  library(slam)
}
if(!require(tm)) {
  install.packages("tm")
  library(tm)
}
if(!require(Rgraphviz)) {
  source("http://bioconductor.org/biocLite.R")
  biocLite("Rgraphviz", ask = FALSE)
  library(Rgraphviz)
}
if(!require(wordcloud)) {
  install.packages("wordcloud")
  library(wordcloud)
}
if(!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}
```

## Processing the data

We Limit the number of docs from each source because the full dataset will require too much computing resource to process.  To do this we stop after reading the first LIMIT_NUM_DOCS lines of each file.  Note that an alternative approach may be to randomly sample lines rather than reading then all.

```{r limit_variable}

LIMIT_NUM_DOCS <- 2000
```

```{r load_data, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
# Create the corpus

## blogs
con <- file('./final/en_US/en_US.blogs.txt') 
open(con)
v <- readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE)
close(con)

# news
con <- file('./final/en_US/en_US.news.txt') 
open(con)
v <- append(v, readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE))
close(con)

# twitter
con <- file('./final/en_US/en_US.twitter.txt') 
open(con)
v <- append(v, readLines(con, n=LIMIT_NUM_DOCS, skipNul=TRUE))
close(con)

vs <- VectorSource(v)

rm(v, con)
```

```{r create_corpus, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
bannedWords <- as.list(read.csv(url('http://www.bannedwordlist.com/lists/swearWords.txt'), header = FALSE))
# head(bannedWords)

blog_corpus <- Corpus(vs, readerControl = list(language="en")) 
blog_corpus <- tm_map(blog_corpus, content_transformer(tolower))
blog_corpus <- tm_map(blog_corpus, stripWhitespace) 
blog_corpus <- tm_map(blog_corpus, removePunctuation) 
blog_corpus <- tm_map(blog_corpus, removeNumbers) 

# the most common n-gram models are based on stop words - we don't want to remove them
# blog_corpus <- tm_map(blog_corpus, removeWords, c(stopwords("english")))

# news corpus has some unicode characters that need to be removed
ct <- content_transformer(function(x, pattern) gsub(pattern, "", x))
blog_corpus <- tm_map(blog_corpus, ct, "\u0095")
blog_corpus <- tm_map(blog_corpus, ct, "\u0096")
blog_corpus <- tm_map(blog_corpus, ct, "\u0097")

# create function to remove words containing a banned word, e.g. *word*

removeWordsContaining <-
function(x, words)
    UseMethod("removeWordsContaining", x)
removeWordsContaining.character <-
function(x, words)
    gsub(sprintf("(*UCP)\\b[a-zA-Z]*(%s)[a-zA-Z]*\\b",
                 paste(sort(words, decreasing = TRUE), collapse = "|")),
         "", x, perl = TRUE)
removeWordsContaining.PlainTextDocument <-
    content_transformer(removeWordsContaining.character)

# filter out the banned words using out removeWordsContaining function
blog_corpus <- tm_map(blog_corpus, removeWordsContaining, bannedWords$V1)

rm(vs, bannedWords, LIMIT_NUM_DOCS)
```

## Wordcloud

It is useful to visualise the wordcloud for the data

```{r word_cloud, warning=FALSE, echo=FALSE}
wordcloud(words = blog_corpus, min.freq = 30)
```

```{r create_term_document_matrix, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
## Tokenize to plain TermDocumentMatrix

tdm <- TermDocumentMatrix(blog_corpus,
                          control = list(removePunctuation = TRUE,
                                         removeNumbers = TRUE,
                                         stopwords = FALSE))
```


```{r first_ten_records_of_tdm, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
# Visualise the TDM, to understand how it is storing the data
inspect(tdm[1:10, 1:10])
```

```{r get_first_50_terms, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
# get a list of the first 50 terms
terms <- dimnames(tdm[ ,1:50])$Terms
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
# View how a common word like 'and' may be distributed in the TDM
inspect(tdm[grep('^and$', terms), 1:10])
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
# Verify that bannedWords have been removed:
inspect(tdm[grep('fuck', terms), 1:10])
```

## N-Grams

### What are the frequencies of 2-grams?

```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
ngramDTM <- function (ngrams) {
  TrigramTokenizer <-
    function(x)
      unlist(lapply(ngrams(words(x), ngrams), paste, collapse = " "), use.names = FALSE)
  
  datmat <- TermDocumentMatrix(blog_corpus, control = list(tokenize = TrigramTokenizer))
  
  return(as.matrix(datmat))
}
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
digrams = ngramDTM(2)
digrams[1:5, 1:3]
```


```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
digramsCounts <- rowSums(digrams)
head(as.matrix(digramsCounts))
# tail(as.matrix(digramsCounts))
```

The top 10 digrams in decending order

```{r, message=FALSE, warning=FALSE, echo=FALSE}
digramsGT10 <- digramsCounts[ digramsCounts > 10 ]
as.matrix (
  digramsGT10[ head( order( digramsGT10, decreasing = T), 10 ) ]
)
```

### What are the frequencies of 3-grams?

The top 10 trigrams in decending order

```{r, message=FALSE, warning=FALSE, echo=FALSE}
trigrams = ngramDTM(3)
trigramsCounts <- rowSums(trigrams)
trigramsGT10 <- trigramsCounts[ trigramsCounts > 10 ]
as.matrix (
  trigramsGT10[ head( order( trigramsGT10, decreasing = T), 10 ) ]
)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
## How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 

#This is a sentence to understand how to count word instances and perform a frequency analysis.
#
# - '2' a
# - '2' to
# - '1' This
# - '1' is
# - '1' sentence
# - '1' understand
# - '1' how
# - '1' count
# - '1' word
# - '1' instances
# - '1' and
# - '1' perform
# - '1' frequency
# - '1' analysis

temp <- inspect(tdm)
FreqMat <- data.frame(ST = rownames(temp), Freq = rowSums(temp))
row.names(FreqMat) <- NULL
```

```{r results="hide", fig.show="hide",  message=FALSE, warning=FALSE, echo=FALSE}
FreqMatDF <- as.data.frame(FreqMat)
FreqMatDF <- FreqMatDF[order(FreqMatDF$Freq, decreasing = TRUE), ]
FreqMatDF$cumsum <- cumsum(FreqMatDF$Freq)
totalWords <- sum(FreqMatDF$Freq)
head(FreqMatDF)
```

## Word count

```{r results="hide", fig.show="hide",  message=FALSE, warning=FALSE, echo=FALSE}

# how many unique terms do we have?  This will be the x axis on the plot
length(unique(FreqMatDF$ST))
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
plot(FreqMatDF$Freq, log="y", type='h', lwd=10, lend=2, xlab="Term ID (ordered by Freq)", ylab="Term Frequency (log scale)", main="Histogram showing distribution of word counts")
```

The above plot shows that a few terms have very high frequency, but over half of the terms only appear once in the corpus.

```{r, message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
ST <- rownames(temp)
plot(FreqMatDF$Freq, log="y", xlab="Frequency Ranked ID", ylab="Frequency of word (log scale)", main="Frequency of terms")
```

50% of corpus is covered by this number of unique words:
```{r, message=FALSE, warning=FALSE, echo=FALSE}
print( nrow(FreqMatDF[FreqMatDF$cumsum < (totalWords * 0.5), ]) )
```

90% of corpus is covered by this number of unique words:
```{r, message=FALSE, warning=FALSE, echo=FALSE}
print( nrow(FreqMatDF[FreqMatDF$cumsum < (totalWords * 0.9), ]) )
```

## Non English words

```{r non_english_words, results="hide", message=FALSE, warning=FALSE, echo=FALSE}
english_words <- as.vector(FreqMatDF$ST[grepl("^[[:digit:][:alpha:]]+$", FreqMatDF$ST)])
head(english_words)
tail(english_words)

non_english_words <- as.vector(FreqMatDF$ST[grepl("^[^[:digit:][:alpha:]]+$", FreqMatDF$ST)])
head(non_english_words)
tail(non_english_words)

num_english_words <- length(english_words)
print( num_english_words )

num_non_english_words <- length(non_english_words)
print( num_non_english_words )
```

Percentage of non-english words

```{r pct_non_english_words, message=FALSE, warning=FALSE, echo=FALSE}
print( 100 * num_non_english_words / (num_english_words + num_non_english_words))
```


# Next Steps

The next steps of the project will be to decide how to create the prediction algorithm.  One option will be to look at the bigram and trigam data to lookup the liklihood of the next term and suggesting the term with the strongest liklihood.  This should be relatively easy to replicate by exporting the bigram and trigram data to a file where it can be re-used in the shiny app.



